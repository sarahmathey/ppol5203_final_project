{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14b222f0-23f0-494c-9054-f02f8e450215",
   "metadata": {},
   "source": [
    "## Scrape State Leg election History "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c61fc64-0929-46df-a838-57a3b56bb98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests \n",
    "import re\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import time # To put the system to sleep\n",
    "import random # for random numbers\n",
    "import json\n",
    "import pandasql as psql\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cab135fc-8fe7-4d2e-8c6f-a0709409ddfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [\n",
    "'Alabama','Alaska','Arizona','Arkansas','California','Colorado','Connecticut','Delaware',\n",
    "'Florida','Georgia','Hawaii','Idaho','Illinois','Indiana','Iowa','Kansas','Kentucky',\n",
    "'Louisiana','Maine','Maryland','Massachusetts','Michigan','Minnesota','Mississippi',\n",
    "'Missouri','Montana','Nebraska','Nevada','New_Hampshire','New_Jersey','New_Mexico',\n",
    "'New_York','North_Carolina','North_Dakota','Ohio','Oklahoma','Oregon','Pennsylvania',\n",
    "'Rhode_Island','South_Carolina','South_Dakota','Tennessee','Texas','Utah','Vermont',\n",
    "'Virginia','Washington','West_Virginia','Wisconsin','Wyoming']\n",
    "\n",
    "#urls = ['https://ballotpedia.org/{}_State_Legislature'.format(state) for state in states]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83321463-044c-45a8-99d1-46c0bbfe54b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ballotpedia_state_senate_control_scraper(state):\n",
    "    '''\n",
    "    for a given state, scrapes the senate election results tables \n",
    "    '''\n",
    "    url = 'https://ballotpedia.org/{}_State_Legislature'.format(state)\n",
    "    page = requests.get(url)\n",
    "    \n",
    "    if page.status_code == 200:\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        senate_table = soup.find('table', {'class': 'wikitable'}).find_next('table')\n",
    "        senate_results = []\n",
    "        for row in senate_table.find_all('tr')[1:]:  # skip the header row\n",
    "            cols = row.find_all('td')\n",
    "            if cols:\n",
    "                senate_results.append([col.text.strip() for col in cols])\n",
    "        \n",
    "        senate_rows = senate_table.find_all('tr')\n",
    "\n",
    "        # Extract headers for Senate\n",
    "        senate_headers = [re.sub(r'[^\\w\\s\\d]', '',header.text.strip())[:2] for header in senate_rows[0].find_all('th')]\n",
    "\n",
    "        list_1 =[]\n",
    "        \n",
    "        for col in senate_headers[1:]:\n",
    "            if col[0]=='9':\n",
    "                list_1.append('19'+col)\n",
    "            else:\n",
    "                list_1.append('20'+col)\n",
    "\n",
    "        first_col = np.where(senate_headers[0]=='Pa','Year','Year')\n",
    "        first_col_string = re.sub(r'[^\\w\\s\\d]', '',np.array2string(first_col, separator=', ').strip('[]'))\n",
    "        \n",
    "        senate_headers = [first_col_string] + list_1\n",
    "\n",
    "        senate_df = pd.DataFrame(senate_results, columns = senate_headers)  # Adjust column names\n",
    "\n",
    "\n",
    "        return(senate_df) \n",
    "    \n",
    "    else:\n",
    "        return({state:page.status_code})\n",
    "\n",
    "def ballotpedia_state_house_control_scraper(state):\n",
    "    '''\n",
    "    for a given state, scrapes the house election results tables \n",
    "    '''\n",
    "    url = 'https://ballotpedia.org/{}_State_Legislature'.format(state)\n",
    "    page = requests.get(url)\n",
    "    \n",
    "    if page.status_code == 200:\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        senate_table = soup.find('table', {'class': 'wikitable'}).find_next('table')\n",
    "\n",
    "# Step 4: Extract House of Representatives election results\n",
    "        house_table = senate_table.find_next('table').find_next('table') \n",
    "        house_results = []\n",
    "        for row in house_table.find_all('tr')[1:]:  # skip the header row\n",
    "            cols = row.find_all('td')\n",
    "            if cols:\n",
    "                house_results.append([col.text.strip() for col in cols])\n",
    "        \n",
    "        house_rows = house_table.find_all('tr')\n",
    "\n",
    "        # Extract headers for House\n",
    "        house_headers = [re.sub(r'[^\\w\\s\\d]', '',header.text.strip())[:2] for header in house_rows[0].find_all('th')]\n",
    "        \n",
    "        list_1 =[]\n",
    "        \n",
    "        for col in house_headers[1:]:\n",
    "            if col[0]=='9':\n",
    "                list_1.append('19'+col)\n",
    "            else:\n",
    "                list_1.append('20'+col)\n",
    "        \n",
    "        first_col = np.where(house_headers[0]=='Pa','Year','Year')\n",
    "        first_col_string = re.sub(r'[^\\w\\s\\d]', '',np.array2string(first_col, separator=', ').strip('[]'))\n",
    "        \n",
    "        house_headers = [first_col_string] + list_1\n",
    "        \n",
    "        house_df = pd.DataFrame(house_results, columns = house_headers)  # Adjust column names\n",
    "\n",
    "        return(house_df) \n",
    "\n",
    "    else:\n",
    "        return({state:page.status_code})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2ef24b2-0c2a-43af-b57a-ba8fbfc76f4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ChunkedEncodingError",
     "evalue": "('Connection broken: IncompleteRead(4505994 bytes read, 695531 more expected)', IncompleteRead(4505994 bytes read, 695531 more expected))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIncompleteRead\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/response.py:748\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 748\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m    750\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    751\u001b[0m     \u001b[38;5;66;03m# FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\u001b[39;00m\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;66;03m# there is yet no clean way to get at it from this context.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/response.py:894\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    884\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    885\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menforce_content_length\n\u001b[1;32m    886\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength_remaining \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;66;03m# raised during streaming, so all calls with incorrect\u001b[39;00m\n\u001b[1;32m    893\u001b[0m         \u001b[38;5;66;03m# Content-Length are caught.\u001b[39;00m\n\u001b[0;32m--> 894\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp_bytes_read, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength_remaining)\n\u001b[1;32m    895\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m read1 \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    896\u001b[0m     (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength_remaining \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(data)\n\u001b[1;32m    897\u001b[0m ):\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    900\u001b[0m     \u001b[38;5;66;03m# `http.client.HTTPResponse`, so we close it here.\u001b[39;00m\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;66;03m# See https://github.com/python/cpython/issues/113199\u001b[39;00m\n",
      "\u001b[0;31mIncompleteRead\u001b[0m: IncompleteRead(4505994 bytes read, 695531 more expected)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/response.py:1060\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1060\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(amt\u001b[38;5;241m=\u001b[39mamt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/response.py:977\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m<\u001b[39m amt \u001b[38;5;129;01mand\u001b[39;00m data:\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;66;03m# TODO make sure to initially read enough data to get past the headers\u001b[39;00m\n\u001b[1;32m    975\u001b[0m     \u001b[38;5;66;03m# For example, the GZ file header takes 10 bytes, we don't want to read\u001b[39;00m\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;66;03m# it one byte at a time\u001b[39;00m\n\u001b[0;32m--> 977\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raw_read(amt)\n\u001b[1;32m    978\u001b[0m     decoded_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode(data, decode_content, flush_decoder)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/response.py:872\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    870\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 872\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[1;32m    873\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp_read(amt, read1\u001b[38;5;241m=\u001b[39mread1) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/contextlib.py:158\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen\u001b[38;5;241m.\u001b[39mthrow(value)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/response.py:772\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    771\u001b[0m         arg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection broken: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 772\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ProtocolError(arg, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    774\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (HTTPException, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mProtocolError\u001b[0m: ('Connection broken: IncompleteRead(4505994 bytes read, 695531 more expected)', IncompleteRead(4505994 bytes read, 695531 more expected))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mChunkedEncodingError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m senate_results\u001b[38;5;241m=\u001b[39m{}\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m state \u001b[38;5;129;01min\u001b[39;00m states:\n\u001b[0;32m----> 3\u001b[0m     df \u001b[38;5;241m=\u001b[39m ballotpedia_state_senate_control_scraper(state)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m      7\u001b[0m        df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mT\n",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m, in \u001b[0;36mballotpedia_state_senate_control_scraper\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mfor a given state, scrapes the senate election results tables \u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      5\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://ballotpedia.org/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_State_Legislature\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(state)\n\u001b[0;32m----> 6\u001b[0m page \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m page\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m      9\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(page\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/sessions.py:746\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[0;32m--> 746\u001b[0m     r\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/models.py:902\u001b[0m, in \u001b[0;36mResponse.content\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 902\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_content(CONTENT_CHUNK_SIZE)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/models.py:822\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 822\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m DecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ContentDecodingError(e)\n",
      "\u001b[0;31mChunkedEncodingError\u001b[0m: ('Connection broken: IncompleteRead(4505994 bytes read, 695531 more expected)', IncompleteRead(4505994 bytes read, 695531 more expected))"
     ]
    }
   ],
   "source": [
    "senate_results={}\n",
    "for state in states:\n",
    "    df = ballotpedia_state_senate_control_scraper(state)\n",
    "    \n",
    "    if df.shape[0]<=3:\n",
    "        \n",
    "       df = df.set_index('Year').T\n",
    "       df['state']=state\n",
    "       df['Democrats'] = pd.to_numeric(df['Democrats'].str.replace(r'\\D+', '', regex=True))\n",
    "       df['Republicans'] = pd.to_numeric(df['Republicans'].str.replace(r'\\D+', '', regex=True))\n",
    "       df['sen_control'] = np.where(df['Democrats']>df['Republicans'],'D','R')\n",
    "       df.reset_index(names='elec_year',inplace=True)\n",
    "       df['elec_year']=pd.to_numeric(df['elec_year'])\n",
    "       df['leg_start'] = df['elec_year']+ 1\n",
    "       df['leg_end'] = df['elec_year'].shift(-1, fill_value=0)\n",
    "    # select only two party results\n",
    "       df=df[['state','Democrats','Republicans','sen_control','elec_year','leg_start','leg_end']]\n",
    "\n",
    "    else: pass \n",
    "\n",
    "    senate_results[state]=df\n",
    "    # Put the system to sleep for a random draw of time (be kind)\n",
    "    time.sleep(random.uniform(.5,1))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a40106e4-8239-4714-83ae-71b30d6d8218",
   "metadata": {},
   "outputs": [],
   "source": [
    "del senate_results['Nebraska']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "651c8d68-489d-45e5-b7ca-36410877031c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred with state: Nebraska\n"
     ]
    }
   ],
   "source": [
    "house_results={}\n",
    "house_states = [state for state in states if state!='Nebraska']\n",
    "for state in house_states:\n",
    "    try:\n",
    "        df = ballotpedia_state_house_control_scraper(state)\n",
    "\n",
    "        if df.shape[0]<=3:\n",
    "           df = df.set_index('Year').T\n",
    "           df['state']=state\n",
    "           df['Democrats'] = pd.to_numeric(df['Democrats'].str.replace(r'\\D+', '', regex=True))\n",
    "           df['Republicans'] = pd.to_numeric(df['Republicans'].str.replace(r'\\D+', '', regex=True))\n",
    "           df['house_control'] = np.where(df['Democrats']>df['Republicans'],'D','R')\n",
    "           df.reset_index(names='elec_year',inplace=True)\n",
    "           df['elec_year']=pd.to_numeric(df['elec_year'])\n",
    "           df['leg_start'] = df['elec_year']+ 1\n",
    "           df['leg_end'] = df['elec_year'].shift(-1, fill_value=0)\n",
    "           df=df[['state','elec_year','Democrats','Republicans','house_control','leg_start','leg_end']]\n",
    "\n",
    "        else: pass \n",
    "    \n",
    "        house_results[state]=df\n",
    "    \n",
    "    except:\n",
    "        print(f'An error occurred with state: {state}')\n",
    "        \n",
    "    # Put the system to sleep for a random draw of time (be kind)\n",
    "    time.sleep(random.uniform(.5,1))\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "43f48fec-4cd8-4c80-bb90-f738c995449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate dictionary values \n",
    "all_house_results = pd.concat(house_results.values(), ignore_index=True)\n",
    "all_senate_results = pd.concat(senate_results.values(), ignore_index=True)\n",
    "\n",
    "# Export the concatenated DataFrame to a CSV file\n",
    "all_house_results.to_csv('house_leg_election_results.csv', index=False)\n",
    "all_senate_results.to_csv('senate_leg_election_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fb7e465-f2ef-4135-b444-96f2be29b85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Years = list(range(1999,2020))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67f45e76-c432-461f-86db-f80e42e61afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the cross product of the two lists\n",
    "combinations = list(itertools.product(Years, states))\n",
    "\n",
    "# Create a DataFrame from the combinations\n",
    "df_base = pd.DataFrame(combinations, columns=['Year', 'state'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ae9e1d7-b722-4ce7-821d-6cb52c562272",
   "metadata": {},
   "outputs": [],
   "source": [
    "gov_data = pd.read_csv('data/project_political_data - gov_party.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2b18f0c-5bb6-435c-b31c-14ad7e2c8ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2024, 2023, 2022, 2021, 2020, 2019, 2018, 2017])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gov_data.year.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f389ae5-ad3a-4971-b6fa-d3005e5ce000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reloading senate and house leg results to not have to re run loop & to update dataset to change year to 2017 instead of 2019 \n",
    "all_senate_results = pd.read_csv('data/senate_leg_election_results.csv')\n",
    "all_house_results = pd.read_csv('data/house_leg_election_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "490fdc97-b6f8-4dd4-a861-f8a1eca1f575",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "select \n",
    "\n",
    "df_base.Year\n",
    ", df_base.state\n",
    "\n",
    ", all_senate_results.elec_year as sen_elec_year\n",
    ", all_senate_results.leg_start as sen_leg_start\n",
    ", all_senate_results.leg_end as sen_leg_end\n",
    ", all_senate_results.sen_control\n",
    "\n",
    ", all_house_results.elec_year as house_elec_year\n",
    ", all_house_results.leg_start as house_leg_start\n",
    ", all_house_results.leg_end as house_leg_end\n",
    ", all_house_results.house_control \n",
    "\n",
    ", gov_data.governor_party\n",
    "\n",
    "from df_base\n",
    "left join all_senate_results \n",
    "    on df_base.state = all_senate_results.state \n",
    "    and df_base.Year between all_senate_results.leg_start and all_senate_results.leg_end\n",
    "left join all_house_results\n",
    "    on df_base.state = all_house_results.state \n",
    "    and df_base.Year between all_house_results.leg_start and all_house_results.leg_end\n",
    "left join gov_data \n",
    "    on df_base.state = gov_data.state \n",
    "    and df_base.Year = gov_data.year\n",
    "\n",
    "group by 1,2,3,4,5,6,7,8,9,10,11\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "results = psql.sqldf(query,locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb08a901-84bc-4c4b-bbf0-552db5991e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009,\n",
       "       2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.Year.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02d5f581-564c-43ec-adf1-22d9c104056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering to 2017 to incorporate a lag indicator of politics \n",
    "political_control_data = results[results['Year']==2017].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73b374be-1deb-4427-af3b-c174f9609145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a boolean variable to indicate whether a state was governed by a gop trifecta \n",
    "political_control_data['gop_trifecta'] = np.where(political_control_data['sen_control']+political_control_data['house_control']+political_control_data['governor_party']=='RRR' ,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "171e43a0-7cb5-423a-acaf-f4a8fcb8959b",
   "metadata": {},
   "outputs": [],
   "source": [
    "political_control_data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28af62be-2a6d-412e-9010-00c26ce13b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update values for Nebraska since unicameral structure messed with flow \n",
    "political_control_data.loc[26,'sen_control']='R'\n",
    "political_control_data.loc[26,'gop_trifecta']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66e567cc-4c77-4b15-915f-d5b26686b9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "political_control_data.to_csv('data/state_gov_control_2017.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc391434-80d0-471c-8cac-a66304be3d47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
